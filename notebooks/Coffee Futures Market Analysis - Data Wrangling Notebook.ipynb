{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffee Market Analysis\n",
    "## Data-Wrangling Notebook\n",
    "\n",
    "### Matthew Garton - February 2019\n",
    "\n",
    "**Purpose:** The purpose of this notebook is to acquire my data, inspect it, clean it and prepare it for EDA and modeling.\n",
    "\n",
    "**Context**: The ultimate goal of my project is to develop trading signals for coffee futures. I will attempt to build a machine learning model which uses fundamental and technical data to predict the future direction of coffee futures price changes. My expectation at the outset of this project is that my feature matrix will include data on weather, GDP, and coffee production and exports in major coffee-producing nations, GDP and coffee import data in major coffee-importing nations, as well as volume, open-interest, and commitment of traders data for ICE coffee futures contracts.\n",
    "\n",
    "Note that many of the decisions made and functions written here came up at various stages of the project, from initial inspection all the way to model-building (as is the non-linear nature of the data science workflow). To keep things clean, I have moved all of the data cleaning/prep (outside of train-test splitting and some feature engineering) to this notebook. The csv file that I output can then be accessed in other notebooks in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering data\n",
    "\n",
    "1. Price data (1973-2019) - daily OHLC prices (plus Volume and OI) for ICE Coffee 'C' futures.\n",
    "\n",
    "    source: [Wiki Continuous Futures database on Quandl](https://www.quandl.com/data/CHRIS-Wiki-Continuous-Futures)\n",
    "      \n",
    "      \n",
    "2. Weather data (1991-2015) - monthly average temperature (celsius) and rainfall (mm) for the top five coffee exporting countries (Brazil, Vietnam, Colombia, Indonesia, Ethiopia).\n",
    "    \n",
    "    source: [World Bank Climate Change Knowledge Portal](http://sdwebx.worldbank.org/climateportal/index.cfm?page=downscaled_data_download&menu=historical)\n",
    "    \n",
    "    \n",
    "3. Fundamental data (1990-2017) - annual data on coffee production, imports, exports, etc. from International Coffee Organization*.\n",
    "\n",
    "    source: [International Coffee Organization](http://www.ico.org/new_historical.asp?section=Statistics)\n",
    "\n",
    "\n",
    "4. Positioning data (1995-2016) - monthly Commitment of Traders' reports from CFTC\n",
    "\n",
    "    source: [Commodity Futures Trading Commission](https://www.cftc.gov/MarketReports/CommitmentsofTraders/HistoricalCompressed/index.htm)\n",
    "    \n",
    "*Note: Before getting started here, I did some initial data assembling/cleaning in excel, so if you choose to get the data directly from the sources listed above, some preparation will be necessary before getting it into the format shown here. The biggest decision I made so far was in how to handle some of the ICO data which was indexed by 'Crop Year' rather than 'Calendar Year'. My initial solution is to treat the most recent year of the 'Crop Year' as the relevant 'year' for the data (so Crop Year 1991/1992 is treated as Year 1992, with the understanding that all of the data for the 1991-1992 period would have been availably by EOY 1992). For now, this is a simplifying assumption to avoid any 'look-ahead bias.' This might be an oversimplification that I'll have to come back to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Daily ICE Coffee 'C' Futures price data\n",
    "coffee = pd.read_csv('../data/CHRIS-ICE_KC1.csv')\n",
    "\n",
    "# import Monthly Weather data for major coffee producing countries\n",
    "weather = pd.read_csv('../data/Weather.csv')\n",
    "\n",
    "# import Annual fundamental (Production, Exports, Imports, etc.) data\n",
    "fundamental = pd.read_csv('../data/SupplyDemand.csv')\n",
    "\n",
    "# import Monthly Commitment of Traders report data\n",
    "cot = pd.read_csv('../data/CommitmentOfTraders.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "While some of the initial data cleaning took place in Excel before importing, there are a few things I need to handle to get these four dataset into one dataframe that will be ready for visualization, EDA, and then feature engineering.\n",
    "\n",
    "In particular, the weather data needs to be reshaped. Because I downloaded separate weather data for each country, the weather data includes separate rows for each Date-Country observation. But I want my data to be indexed by Date for modeling, with the specific country data to represent features of that observation, rather than separate observations themselves. To accomplish this, I use a very simple version of the 'Split-Apply-Combine' paradigm. I separate each country's weather data into it's own smaller dataframe, rename the relevant coluns to be tagged with country name, then combine them back into one dataframe indexed on Date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick fix to 'Country' column typo..\n",
    "weather.rename(index=str, columns={' Country':'Country'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataframe, index by Date (as datetime object) and extract year, month\n",
    "dfs = [coffee, weather, fundamental, cot]\n",
    "for df in dfs:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    \n",
    "# coffee and cot are sorted backwards; reverse order\n",
    "coffee.sort_index(inplace=True)\n",
    "cot.sort_index(inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For weather data, what I want is one row per observation, with each country's\n",
    "# data represented in columns of that row\n",
    "\n",
    "countries = ['BRA', 'COL', 'ETH', 'IDN', 'VNM']\n",
    "\n",
    "# split weather into dfs for each country and rename columns appropriately\n",
    "dfs = []\n",
    "for country in countries:\n",
    "    df = weather[weather['Country'] == country]\n",
    "    df.rename(index=str, \n",
    "              columns={'Temperature (Monthly â€“ C)':'{}_Temp'.format(country),\n",
    "                       'Precip (mm)':'{}_Precip'.format(country)}, inplace=True)\n",
    "    df.drop(columns=['Country'], inplace=True)\n",
    "    dfs.append(df)\n",
    "\n",
    "# combine separate countries' weather data into one frame indexed by date\n",
    "weather = dfs[0]\n",
    "\n",
    "for df in dfs[1:]:\n",
    "    cols = df.columns.difference(weather.columns)\n",
    "    weather = weather.merge(df[cols], left_index=True, right_index=True, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.index = pd.to_datetime(weather.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Finally, I combine the four individual dataframes into one master dataframe which feeds into the next stage of my workflow - EDA. There is a slight problem with the periodicity of the data, in that my explanatory variables are not as granular as what I'm trying to predict - prices. This is not ideal and also presents a technical problem when thinking about creating a feature matrix (i.e. lots of NaN values). From a modeling perspective, I may need to circle back to try and get more granular data (daily weather data, for example). However, this level of granularity could be unnecessary for my purposes. I will handle the practical aspect of filling in NaN values in the next stage, but for now I will maintain the integrity of the data (i.e. only export the data that I have). This will make it easier to test out different methods of cleaning up the data in future stages, without being locked into a particular decision now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all data into one dataframe\n",
    "dfs = [coffee, weather, fundamental, cot]\n",
    "\n",
    "full_data = pd.concat(dfs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Settle', 'Change', 'Wave', 'Volume',\n",
       "       'Prev. Day Open Interest', 'EFP Volume', 'EFS Volume', 'Block Volume',\n",
       "       'BRA_Temp', 'BRA_Precip', 'COL_Precip', 'COL_Temp', 'ETH_Precip',\n",
       "       'ETH_Temp', 'IDN_Precip', 'IDN_Temp', 'VNM_Precip', 'VNM_Temp', 'Year',\n",
       "       'Production', 'Consumption (domestic)', 'Exportable Production',\n",
       "       'Gross Opening Stocks', 'Exports', 'Imports', 'Re-exports',\n",
       "       'Inventories', 'Disappearance', 'Open_Interest_All',\n",
       "       'NonComm_Positions_Long_All', 'NonComm_Positions_Short_All',\n",
       "       'NonComm_Postions_Spread_All', 'Comm_Positions_Long_All',\n",
       "       'Comm_Positions_Short_All', 'Tot_Rept_Positions_Long_All',\n",
       "       'Tot_Rept_Positions_Short_All', 'NonRept_Positions_Long_All',\n",
       "       'NonRept_Positions_Short_All', 'Pct_of_OI_NonComm_Long_All',\n",
       "       'Pct_of_OI_NonComm_Short_All', 'Pct_of_OI_NonComm_Spread_All',\n",
       "       'Pct_of_OI_Comm_Long_All', 'Pct_of_OI_Comm_Short_All',\n",
       "       'Pct_of_OI_Tot_Rept_Long_All', 'Pct_of_OI_Tot_Rept_Short_All',\n",
       "       'Pct_of_OI_NonRept_Long_All', 'Pct_of_OI_NonRept_Short_All',\n",
       "       'Conc_Gross_LE_4_TDR_Long_All', 'Conc_Gross_LE_4_TDR_Short_All',\n",
       "       'Conc_Gross_LE_8_TDR_Long_All', 'Conc_Gross_LE_8_TDR_Short_All',\n",
       "       'Conc_Net_LE_4_TDR_Long_All', 'Conc_Net_LE_4_TDR_Short_All',\n",
       "       'Conc_Net_LE_8_TDR_Long_All', 'Conc_Net_LE_8_TDR_Short_All'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of data where datasets overlap (~1995-2015)\n",
    "coffee_data = full_data['1994':'2016']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "cols_to_drop = ['Wave', 'Prev. Day Open Interest',\n",
    "                'EFP Volume', 'EFS Volume', 'Block Volume',\n",
    "                'Year']\n",
    "\n",
    "coffee_data.drop(columns=cols_to_drop, inplace=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "coffee_data.to_csv('../data/CoffeeDataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
